{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "import gensim\n",
    "import sklearn\n",
    "from sympy.parsing.sympy_parser import parse_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     C:\\Users\\chatu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "nltk.download('word2vec_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLM:\n",
    "\tdef __init__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tN-gram Language Model\n",
    "\t\t\"\"\"\n",
    "\t\t# Dictionary to store next-word possibilities for bigrams. Maintains a list for each bigram.\n",
    "\t\tself.bigram_prefix_to_trigram = {}\n",
    "\t\t\n",
    "\t\t# Dictionary to store counts of corresponding next-word possibilities for bigrams. Maintains a list for each bigram.\n",
    "\t\tself.bigram_prefix_to_trigram_weights = {}\n",
    "\n",
    "\tdef load_trigrams(self):\n",
    "\t\t\"\"\"\n",
    "\t\tLoads the trigrams from the data file and fills the dictionaries defined above.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\t\"\"\"\n",
    "\t\twith open(\"data/tweets/covid-tweets-2020-08-10-2020-08-21.trigrams.txt\", encoding=\"utf-8\") as f:\n",
    "\t\t\tlines = f.readlines()\n",
    "\t\t\tfor i, line in enumerate(lines):\n",
    "\t\t\t\tword1, word2, word3, count = line.strip().split()\n",
    "\t\t\t\tif (word1, word2) not in self.bigram_prefix_to_trigram:\n",
    "\t\t\t\t\tself.bigram_prefix_to_trigram[(word1, word2)] = []\n",
    "\t\t\t\t\tself.bigram_prefix_to_trigram_weights[(word1, word2)] = []\n",
    "\t\t\t\tself.bigram_prefix_to_trigram[(word1, word2)].append(word3)\n",
    "\t\t\t\tself.bigram_prefix_to_trigram_weights[(word1, word2)].append(int(count))\n",
    "\n",
    "\tdef top_next_word(self, word1, word2, n=10):\n",
    "\t\t\"\"\"\n",
    "\t\tRetrieve top n next words and their probabilities given a bigram prefix.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tword1: str\n",
    "\t\t\tThe first word in the bigram.\n",
    "\t\tword2: str\n",
    "\t\t\tThe second word in the bigram.\n",
    "\t\tn: int\n",
    "\t\t\tNumber of words to return.\n",
    "\t\t\t\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tnext_words: list\n",
    "\t\t\tThe retrieved top n next words.\n",
    "\t\tprobs: list\n",
    "\t\t\tThe probabilities corresponding to the retrieved words.\n",
    "\t\t\"\"\"\n",
    "\t\tnext_words = []\n",
    "\t\tprobs = []\n",
    "\n",
    "    # Check if the bigram exists in our dictionary\n",
    "\t\tif (word1, word2) in self.bigram_prefix_to_trigram:\n",
    "\t\t\t# Get the list of next words and their counts\n",
    "\t\t\twords = self.bigram_prefix_to_trigram[(word1, word2)]\n",
    "\t\t\tweights = self.bigram_prefix_to_trigram_weights[(word1, word2)]\n",
    "\t\t\t\n",
    "\t\t\t# Calculate the total count to get probabilities\n",
    "\t\t\ttotal_count = sum(weights)\n",
    "\t\t\t\n",
    "\t\t\t# Create a list of (word, probability) tuples\n",
    "\t\t\tword_probs = [(word, count/total_count) for word, count in zip(words, weights)]\n",
    "\t\t\t\n",
    "\t\t\t# Sort by probability in descending order\n",
    "\t\t\tword_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "\t\t\t\n",
    "\t\t\t# Take the top n results\n",
    "\t\t\ttop_results = word_probs[:n]\n",
    "\t\t\t\n",
    "\t\t\t# Separate words and probabilities into two lists\n",
    "\t\t\tnext_words = [w for w, p in top_results]\n",
    "\t\t\tprobs = [p for w, p in top_results]\n",
    "\n",
    "\t\t\treturn next_words, probs\n",
    "\t\n",
    "\tdef sample_next_word(self, word1, word2, n=10):\n",
    "\t\t\"\"\"\n",
    "\t\tSample n next words and their probabilities given a bigram prefix using the probability distribution defined by frequency counts.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tword1: str\n",
    "\t\t\tThe first word in the bigram.\n",
    "\t\tword2: str\n",
    "\t\t\tThe second word in the bigram.\n",
    "\t\tn: int\n",
    "\t\t\tNumber of words to return.\n",
    "\t\t\t\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tnext_words: list\n",
    "\t\t\tThe sampled n next words.\n",
    "\t\tprobs: list\n",
    "\t\t\tThe probabilities corresponding to the retrieved words.\n",
    "\t\t\"\"\"\n",
    "\t\tnext_words = []\n",
    "\t\tprobs = []\n",
    "\n",
    "\t\t# Check if the bigram exists in our dictionary\n",
    "\t\tif (word1, word2) in self.bigram_prefix_to_trigram:\n",
    "\t\t\t# Get the list of next words and their counts\n",
    "\t\t\twords = self.bigram_prefix_to_trigram[(word1, word2)]\n",
    "\t\t\tweights = self.bigram_prefix_to_trigram_weights[(word1, word2)]\n",
    "\t\t\t\n",
    "\t\t\t# Calculate the total count to get probabilities\n",
    "\t\t\ttotal_count = sum(weights)\n",
    "\t\t\t\n",
    "\t\t\t# Create probabilities array for numpy sampling\n",
    "\t\t\tprobabilities = [count/total_count for count in weights]\n",
    "\t\t\t\n",
    "\t\t\t# Sample without replacement\n",
    "\t\t\timport numpy as np\n",
    "\t\t\t# Limit sample size to available words if less than n\n",
    "\t\t\tsample_size = min(n, len(words))\n",
    "\t\t\tindices = np.random.choice(len(words), size=sample_size, replace=False, p=probabilities)\n",
    "\t\t\t\n",
    "\t\t\t# Get the sampled words and their probabilities\n",
    "\t\t\tnext_words = [words[i] for i in indices]\n",
    "\t\t\tprobs = [probabilities[i] for i in indices]\n",
    "    \n",
    "\t\treturn next_words, probs\n",
    "\t\n",
    "\tdef generate_sentences(self, prefix, beam=10, sampler=None, max_len=20):\n",
    "\t\t\"\"\"\n",
    "\t\tGenerate sentences using beam search.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tprefix: str\n",
    "\t\t\tString containing two (or more) words separated by spaces.\n",
    "\t\tbeam: int\n",
    "\t\t\tThe beam size.\n",
    "\t\tsampler: Callable\n",
    "\t\t\tThe function used to sample next word.\n",
    "\t\tmax_len: int\n",
    "\t\t\tMaximum length of sentence (as measure by number of words) to generate (excluding \"<EOS>\").\n",
    "\t\t\t\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tsentences: list\n",
    "\t\t\tThe top generated sentences\n",
    "\t\tprobs: list\n",
    "\t\t\tThe probabilities corresponding to the generated sentences\n",
    "\t\t\"\"\"\n",
    "\t\tsentences = []\n",
    "\t\tprobs = []\n",
    "\t\t\n",
    "\t\t# If sampler is not provided, use top_next_word as default\n",
    "\t\tif sampler is None:\n",
    "\t\t\tsampler = self.top_next_word\n",
    "\t\t\t\n",
    "\t\t# Process the prefix to get starting words\n",
    "\t\tprefix_words = prefix.strip().split()\n",
    "\t\t\n",
    "\t\t# Need at least 2 words to start\n",
    "\t\tif len(prefix_words) < 2:\n",
    "\t\t\treturn sentences, probs\n",
    "\t\t\n",
    "\t\t# Initialize beam search with the given prefix\n",
    "\t\tbeam_sentences = [prefix_words]\n",
    "\t\tbeam_probs = [1.0]  # Starting probability of 1\n",
    "\t\t\n",
    "\t\t# Continue until all sentences in beam have ended\n",
    "\t\twhile not all(\"<EOS>\" in sentence for sentence in beam_sentences):\n",
    "\t\t\t# Track candidate extensions for this round\n",
    "\t\t\tcandidates = []\n",
    "\t\t\tcandidate_probs = []\n",
    "\t\t\t\n",
    "\t\t\t# Process each sentence in the current beam\n",
    "\t\t\tfor i, sentence in enumerate(beam_sentences):\n",
    "\t\t\t\t# Skip if sentence already ended\n",
    "\t\t\t\tif \"<EOS>\" in sentence:\n",
    "\t\t\t\t\t# Keep this completed sentence as a candidate\n",
    "\t\t\t\t\tcandidates.append(sentence)\n",
    "\t\t\t\t\tcandidate_probs.append(beam_probs[i])\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t# Count words after the prefix\n",
    "\t\t\t\t# The prefix itself doesn't count toward the maximum length\n",
    "\t\t\t\twords_after_prefix = len(sentence) - len(prefix_words)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Check if we've reached maximum length\n",
    "\t\t\t\tif words_after_prefix >= max_len:\n",
    "\t\t\t\t\t# Add EOS and don't extend further\n",
    "\t\t\t\t\tcandidates.append(sentence + [\"<EOS>\"])\n",
    "\t\t\t\t\tcandidate_probs.append(beam_probs[i])\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t# Get last two words to predict next word\n",
    "\t\t\t\tword1, word2 = sentence[-2], sentence[-1]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Get next word predictions using the provided sampler\n",
    "\t\t\t\tnext_words, next_probs = sampler(word1, word2)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# If no predictions, end the sentence\n",
    "\t\t\t\tif not next_words:\n",
    "\t\t\t\t\tcandidates.append(sentence + [\"<EOS>\"])\n",
    "\t\t\t\t\tcandidate_probs.append(beam_probs[i])\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t# Add all extensions to candidates\n",
    "\t\t\t\tfor j, next_word in enumerate(next_words):\n",
    "\t\t\t\t\t# If the next word is <EOS>, add it\n",
    "\t\t\t\t\tif next_word == \"<EOS>\":\n",
    "\t\t\t\t\t\tnew_sentence = sentence + [next_word]\n",
    "\t\t\t\t\t\tnew_prob = beam_probs[i] * next_probs[j]\n",
    "\t\t\t\t\t\tcandidates.append(new_sentence)\n",
    "\t\t\t\t\t\tcandidate_probs.append(new_prob)\n",
    "\t\t\t\t\t# Otherwise, check if adding this word would exceed max_len\n",
    "\t\t\t\t\telif words_after_prefix + 1 < max_len:\n",
    "\t\t\t\t\t\tnew_sentence = sentence + [next_word]\n",
    "\t\t\t\t\t\tnew_prob = beam_probs[i] * next_probs[j]\n",
    "\t\t\t\t\t\tcandidates.append(new_sentence)\n",
    "\t\t\t\t\t\tcandidate_probs.append(new_prob)\n",
    "\t\t\t\t\t# If adding this word would make the sentence exactly max_len, add EOS\n",
    "\t\t\t\t\telif words_after_prefix + 1 == max_len:\n",
    "\t\t\t\t\t\tnew_sentence = sentence + [next_word, \"<EOS>\"]\n",
    "\t\t\t\t\t\tnew_prob = beam_probs[i] * next_probs[j]\n",
    "\t\t\t\t\t\tcandidates.append(new_sentence)\n",
    "\t\t\t\t\t\tcandidate_probs.append(new_prob)\n",
    "\t\t\t\n",
    "\t\t\t# If no candidates, break\n",
    "\t\t\tif not candidates:\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\t# Sort candidates by probability (descending)\n",
    "\t\t\tsorted_indices = sorted(range(len(candidate_probs)), key=lambda i: candidate_probs[i], reverse=True)\n",
    "\t\t\t\n",
    "\t\t\t# Select top beam candidates\n",
    "\t\t\tbeam_sentences = [candidates[i] for i in sorted_indices[:beam]]\n",
    "\t\t\tbeam_probs = [candidate_probs[i] for i in sorted_indices[:beam]]\n",
    "\t\t\n",
    "\t\t# Convert word lists to sentences\n",
    "\t\tsentences = [' '.join(sentence) for sentence in beam_sentences]\n",
    "\t\tprobs = beam_probs\n",
    "\t\t\n",
    "\t\treturn sentences, probs\n",
    "\t\n",
    "# Define your language model object\n",
    "language_model = NgramLM()\n",
    "# Load trigram data\n",
    "language_model.load_trigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluating top next word prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.807981220657277\n",
      "the 0.06948356807511737\n",
      "pandemic 0.023943661971830985\n",
      "this 0.016901408450704224\n",
      "an 0.0107981220657277\n",
      "covid 0.009389671361502348\n",
      "nowhere 0.008450704225352112\n",
      "it 0.004694835680751174\n",
      "summer 0.002347417840375587\n",
      "lockdown 0.002347417840375587\n"
     ]
    }
   ],
   "source": [
    "next_words, probs = language_model.top_next_word(\"middle\", \"of\", 10)\n",
    "for word, prob in zip(next_words, probs):\n",
    "\tprint(word, prob)\n",
    "# Your first 5 lines of output should be exactly:\n",
    "# a 0.807981220657277\n",
    "# the 0.06948356807511737\n",
    "# pandemic 0.023943661971830985\n",
    "# this 0.016901408450704224\n",
    "# an 0.0107981220657277"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluating sample next word prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.807981220657277\n",
      "nowhere 0.008450704225352112\n",
      "pandemic 0.023943661971830985\n",
      "the 0.06948356807511737\n",
      "august 0.0018779342723004694\n",
      "an 0.0107981220657277\n",
      "this 0.016901408450704224\n",
      "flu 0.00046948356807511736\n",
      "covid 0.009389671361502348\n",
      "summer 0.002347417840375587\n"
     ]
    }
   ],
   "source": [
    "next_words, probs = language_model.sample_next_word(\"middle\", \"of\", 10)\n",
    "for word, prob in zip(next_words, probs):\n",
    "\tprint(word, prob)\n",
    "# My first 5 lines of output look like this: (YOUR OUTPUT CAN BE DIFFERENT!)\n",
    "# a 0.807981220657277\n",
    "# pandemic 0.023943661971830985\n",
    "# august 0.0018779342723004694\n",
    "# stage 0.0018779342723004694\n",
    "# an 0.0107981220657277"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluating beam search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS1> <BOS2> trump eyes new unproven coronavirus treatment URL <EOS> 0.00021893147502903603\n",
      "<BOS1> <BOS2> trump eyes new unproven coronavirus cure URL <EOS> 0.0001719607222046247\n",
      "<BOS1> <BOS2> trump eyes new unproven virus cure promoted by mypillow ceo over unproven therapeutic URL <EOS> 9.773272077557522e-05\n",
      "<BOS1> <BOS2> trump eyes new unproven coronavirus therapeutic mypillow creator over unproven therapeutic URL <EOS> 8.212549111137046e-05\n",
      "<BOS1> <BOS2> trump eyes new unproven virus cure promoted by ben carson and mypillow founder URL <EOS> 1.2095697936835552e-05\n",
      "<BOS1> <BOS2> trump eyes new unproven virus cure promoted by mypillow ceo over unproven therapeutic URL via @USER <EOS> 7.432226908194607e-06\n",
      "<BOS1> <BOS2> trump eyes new unproven virus cure promoted by mypillow ceo over unproven and dangerous <EOS> 5.61685494684627e-06\n",
      "<BOS1> <BOS2> trump eyes new unproven virus cure promoted by mypillow ceo over unproven and dangerous covid-19 treatment URL <EOS> 5.235550241426875e-06\n",
      "<BOS1> <BOS2> trump eyes new unproven virus cure promoted by ben carson and mypillow founder and ceo of mypillow URL <EOS> 2.1484173056680325e-06\n",
      "<BOS1> <BOS2> trump eyes new unproven virus cure promoted by ben carson and mypillow founder and ceo mike lindell a big deal and <EOS> 1.901809132652209e-08\n",
      "#########################\n",
      "\n",
      "<BOS1> <BOS2> biden calls for a 30 bonus URL #cashgem #cashappfriday #stayathome <EOS> 0.0002495268686322749\n",
      "<BOS1> <BOS2> biden says all u.s. governors should mandate masks <EOS> 1.6894510541025754e-05\n",
      "<BOS1> <BOS2> biden says all u.s. governors question cost of a pandemic <EOS> 8.777606198953028e-07\n",
      "<BOS1> <BOS2> biden says all u.s. governors should mandate mandatory mask rule URL <EOS> 6.46094976762742e-07\n",
      "<BOS1> <BOS2> biden says all u.s. governors should mandate masks and social distancing <EOS> 4.6833316176693136e-07\n",
      "<BOS1> <BOS2> biden says all u.s. governors should mandate mandatory mask wearing and social distancing <EOS> 4.380454675651422e-08\n",
      "<BOS1> <BOS2> biden says all u.s. governors should mandate mandatory mask wearing and social distancing URL <EOS> 2.2277082512658355e-08\n",
      "<BOS1> <BOS2> biden says all u.s. governors should mandate mandatory mask wearing and social distancing and wearing masks <EOS> 1.166766912614153e-10\n",
      "<BOS1> <BOS2> biden says all u.s. governors should mandate mandatory mask wearing and social distancing and wearing a mask <EOS> 1.1355525098965152e-10\n",
      "<BOS1> <BOS2> biden says all u.s. governors should mandate mandatory mask wearing and social distancing and wearing a mask and social distancing and <EOS> 8.88591344786591e-13\n",
      "#########################\n",
      "\n",
      "<BOS1> <BOS2> trump backs violent qanon cultists at white house URL <EOS> 9.29780910057451e-05\n",
      "<BOS1> <BOS2> trump backs violent qanon cultists at white house <EOS> 6.116014467537672e-05\n",
      "<BOS1> <BOS2> trump backs violent qanon cultists at white house URL URL <EOS> 6.580186926318646e-06\n",
      "<BOS1> <BOS2> trump backs violent qanon cultists at white house coronavirus failures URL <EOS> 6.535253432066961e-06\n",
      "<BOS1> <BOS2> trump backs violent qanon cultists at white hart lane between 2007 and 2013 before leaving washington on recess during a pandemic <EOS> 1.4684581343354725e-07\n",
      "<BOS1> <BOS2> trump backs violent qanon cultists at white hart lane between 2007 and 2013 before leaving washington on both sides of the <EOS> 1.0634014342472478e-07\n",
      "<BOS1> <BOS2> trump backs violent qanon cultists at white hart lane between 2007 and 2013 before leaving washington on recess without deal on <EOS> 7.821146032528146e-08\n",
      "<BOS1> <BOS2> trump backs violent qanon cultists at white hart lane between 2007 and 2013 before leaving washington on recess until september without <EOS> 3.4370270650758456e-08\n",
      "<BOS1> <BOS2> trump backs violent qanon cultists at white hart lane between 2007 and 2013 before leaving washington on recess during the pandemic <EOS> 2.6422830383952706e-08\n",
      "<BOS1> <BOS2> trump backs violent qanon cultists at white hart lane between 2007 and 2013 before leaving washington on recess during a global <EOS> 1.4241590981214132e-08\n",
      "#########################\n",
      "\n",
      "<BOS1> <BOS2> biden does n't have to wear a mask <EOS> 1.227865423838642e-06\n",
      "<BOS1> <BOS2> biden does n’t have to wear a mask <EOS> 1.0430322974384168e-06\n",
      "<BOS1> <BOS2> biden wants to go back to normal <EOS> 7.091276439304324e-07\n",
      "<BOS1> <BOS2> biden does n't have to wear a mask URL <EOS> 4.6488761704576076e-07\n",
      "<BOS1> <BOS2> biden wants to go back to school during a pandemic <EOS> 5.810166705384119e-08\n",
      "<BOS1> <BOS2> biden does n't have to wear a mask and social distancing <EOS> 7.758409581932596e-09\n",
      "<BOS1> <BOS2> biden does n't have to wear a mask and social distance and wear a mask <EOS> 5.0593808586663876e-11\n",
      "<BOS1> <BOS2> biden does n't have to wear a mask and social distancing and wearing masks <EOS> 2.0665105028991966e-11\n",
      "<BOS1> <BOS2> biden does n't have to wear a mask and social distance and wear a mask URL <EOS> 1.9155548038474953e-11\n",
      "<BOS1> <BOS2> biden does n't have to wear a mask and social distance and wear a mask in public <EOS> 4.27822408024513e-13\n"
     ]
    }
   ],
   "source": [
    "sentences, probs = language_model.generate_sentences(prefix=\"<BOS1> <BOS2> trump\", beam=10, sampler=language_model.top_next_word)\n",
    "for sent, prob in zip(sentences, probs):\n",
    "\tprint(sent, prob)\n",
    "print(\"#########################\\n\")\n",
    "# Your first 3 lines of output should be exactly:\n",
    "# <BOS1> <BOS2> trump eyes new unproven coronavirus treatment URL <EOS> 0.00021893147502903603\n",
    "# <BOS1> <BOS2> trump eyes new unproven coronavirus cure URL <EOS> 0.0001719607222046247\n",
    "# <BOS1> <BOS2> trump eyes new unproven virus cure promoted by mypillow ceo over unproven therapeutic URL <EOS> 9.773272077557522e-05\n",
    "\n",
    "sentences, probs = language_model.generate_sentences(prefix=\"<BOS1> <BOS2> biden\", beam=10, sampler=language_model.top_next_word)\n",
    "for sent, prob in zip(sentences, probs):\n",
    "\tprint(sent, prob)\n",
    "print(\"#########################\\n\")\n",
    "# Your first 3 lines of output should be exactly:\n",
    "# <BOS1> <BOS2> biden calls for a 30 bonus URL #cashgem #cashappfriday #stayathome <EOS> 0.0002495268686322749\n",
    "# <BOS1> <BOS2> biden says all u.s. governors should mandate masks <EOS> 1.6894510541025754e-05\n",
    "# <BOS1> <BOS2> biden says all u.s. governors question cost of a pandemic <EOS> 8.777606198953028e-07\n",
    "\n",
    "sentences, probs = language_model.generate_sentences(prefix=\"<BOS1> <BOS2> trump\", beam=10, sampler=language_model.sample_next_word)\n",
    "for sent, prob in zip(sentences, probs):\n",
    "\tprint(sent, prob)\n",
    "print(\"#########################\\n\")\n",
    "# My first 3 lines of output look like this: (YOUR OUTPUT CAN BE DIFFERENT!)\n",
    "# <BOS1> <BOS2> trump eyes new unproven coronavirus treatment URL <EOS> 0.00021893147502903603\n",
    "# <BOS1> <BOS2> trump eyes new unproven coronavirus cure URL <EOS> 0.0001719607222046247\n",
    "# <BOS1> <BOS2> trump eyes new unproven virus cure promoted by mypillow ceo over unproven therapeutic URL <EOS> 9.773272077557522e-05\n",
    "\n",
    "sentences, probs = language_model.generate_sentences(prefix=\"<BOS1> <BOS2> biden\", beam=10, sampler=language_model.sample_next_word)\n",
    "for sent, prob in zip(sentences, probs):\n",
    "\tprint(sent, prob)\n",
    "# My first 3 lines of output look like this: (YOUR OUTPUT CAN BE DIFFERENT!)\n",
    "# <BOS1> <BOS2> biden is elected <EOS> 0.001236227651321991\n",
    "# <BOS1> <BOS2> biden dropping ten points given trump a confidence trickster URL <EOS> 5.1049579351466146e-05\n",
    "# <BOS1> <BOS2> biden dropping ten points given trump four years <EOS> 4.367575122292103e-05"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
