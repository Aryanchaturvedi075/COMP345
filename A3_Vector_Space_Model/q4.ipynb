{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\chatu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chatu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.tagged import TaggedCorpusView\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [brown.words(fileid) for fileid in brown.fileids()]\n",
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_matrix(documents: list[TaggedCorpusView], stopwords: list[str]) -> tuple[np.ndarray, list[str]]:\n",
    "    # Preprocessing documents\n",
    "    processed_docs = [\n",
    "        [word.lower() for word in doc if word.isalnum() and word.lower() not in stopwords] \n",
    "        for doc in documents\n",
    "    ]\n",
    "    # Create sorted vocabulary and index mapping\n",
    "    vocabulary = sorted({word for doc in processed_docs for word in doc})\n",
    "    vocab_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    num_docs, num_words = len(documents), len(vocabulary)\n",
    "    \n",
    "    # Initialize term_frequency_matrix and document frequencies\n",
    "    tf_matrix = np.zeros((num_docs, num_words))\n",
    "    doc_freq = np.zeros(num_words)\n",
    "    \n",
    "    for i, doc in enumerate(processed_docs):\n",
    "        word_counts = Counter(doc)\n",
    "        for word, count in word_counts.items():\n",
    "            j = vocab_index[word]\n",
    "            tf_matrix[i, j] = count\n",
    "            doc_freq[j] += 1\n",
    "    \n",
    "    # Compute smoothened IDF using formula log10(N / (df + 1)) + 1\n",
    "    idf = np.log10(num_docs / (doc_freq + 1)) + 1\n",
    "    tfidf_matrix = tf_matrix * idf\n",
    "    \n",
    "    return tfidf_matrix, vocabulary\n",
    "\n",
    "\n",
    "def get_idf_values(documents : list[TaggedCorpusView], stopwords: list[str]) -> np.ndarray:\n",
    "    processed_docs = [\n",
    "        [word.lower() for word in doc if word.isalnum() and word.lower() not in stopwords] \n",
    "        for doc in documents\n",
    "    ]\n",
    "    vocabulary = sorted({word for doc in processed_docs for word in doc})\n",
    "    vocab_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    doc_freq = np.zeros(len(vocabulary))\n",
    "    \n",
    "    for doc in processed_docs:\n",
    "        word_counts = Counter(doc)\n",
    "        for word, _ in word_counts.items():\n",
    "            j = vocab_index[word]\n",
    "            doc_freq[j] += 1\n",
    "    \n",
    "    idf = np.log10(len(documents) / (doc_freq + 1)) + 1\n",
    "    return idf\n",
    "\n",
    "\n",
    "def calculate_sparsity(tfidf_matrix: np.ndarray) -> float:\n",
    "    sparsity = (tfidf_matrix == 0).sum() / tfidf_matrix.size\n",
    "    return sparsity\n",
    "\n",
    "\n",
    "def extract_salient_words(VT: np.ndarray, vocabulary: list[str], k: int) -> dict[int, list[str]]:\n",
    "    n = VT.shape[0]\n",
    "    salient_words = {}\n",
    "    \n",
    "    for dim in range(n):        \n",
    "        # Sort the weights of each latent dimension\n",
    "        sorted_indices = np.argsort(VT[dim, :])\n",
    "        top_k_indices = sorted_indices[-k:]\n",
    "        \n",
    "        # Convert indices to words\n",
    "        salient_words[dim] = [vocabulary[idx] for idx in top_k_indices]\n",
    "    \n",
    "    return salient_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(A: np.ndarray, B: np.ndarray) -> float:\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm1 = np.linalg.norm(A)\n",
    "    norm2 = np.linalg.norm(B)\n",
    "    cosine_similarity = 0.0 if norm1 == 0 or norm2 == 0 else dot_product / (norm1 * norm2)\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "def get_similar_documents(U: np.ndarray, Sigma: np.ndarray, VT: np.ndarray, doc_index: int, k: int) -> list[int]:\n",
    "    doc_embeddings = U * Sigma\n",
    "    query_embedding = doc_embeddings[doc_index]\n",
    "    \n",
    "    # Calculate cosine similarity between query document and all documents\n",
    "    similarities = []\n",
    "    for i in range(len(doc_embeddings)):\n",
    "        if i != doc_index:  # Exclude the query document\n",
    "            sim = cos_sim(query_embedding, doc_embeddings[i])\n",
    "            similarities.append((i, sim))\n",
    "    \n",
    "    # Sort by similarity in descending order and get top k\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    similar_doc_indices = [idx for idx, _ in similarities[:k]]\n",
    "    \n",
    "    return similar_doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_retrieval(vocabulary: list[str], idf_values: np.ndarray, U: np.ndarray, Sigma: np.ndarray, VT: np.ndarray, query: list[str], k: int) -> list[int]:\n",
    "    # Create vocabulary index mapping and process query\n",
    "    vocab_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    query_counts = Counter(w.lower() for w in query if w.isalnum())\n",
    "    \n",
    "    # Construct query TF-IDF vector (vocabulary size)\n",
    "    q_tfidf = np.zeros(len(vocabulary))\n",
    "    for word, count in query_counts.items():\n",
    "        if word in vocab_index:\n",
    "            q_tfidf[vocab_index[word]] = count * idf_values[vocab_index[word]]\n",
    "    \n",
    "    # Project query into 10-dimensional LSA space\n",
    "    # q_tfidf (1 × vocab_size) @ VT.T (vocab_size × 10) = (1 × 10)\n",
    "    q_lsa = q_tfidf @ VT.T\n",
    "    \n",
    "    # Scale query by singular values\n",
    "    q_embedding = q_lsa / Sigma\n",
    "    \n",
    "    # Get document vectors in LSA space\n",
    "    # U (n_docs × 10) already represents documents in LSA space\n",
    "    doc_embeddings = U * Sigma  # Broadcasting Sigma across columns\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    similarities = np.array([cos_sim(q_embedding, doc) for doc in doc_embeddings])\n",
    "    \n",
    "    # Return indices of top k most similar documents\n",
    "    return similarities.argsort()[-k:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 40881)\n",
      "[5.96857651 2.1079054  3.         2.07572071 2.69897   ]\n",
      "['amoral', 'amorality', 'amorist', 'amorous', 'amorphous', 'amorphously', 'amortization', 'amortize', 'amory', 'amos']\n",
      "0.9845266994447298\n"
     ]
    }
   ],
   "source": [
    "# This will take a few minutes to run\n",
    "tfidf_matrix, vocabulary = create_tfidf_matrix(documents, stopwords_list)\n",
    "idf_values = get_idf_values(documents, stopwords_list)\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "# (500, 40881)\n",
    "print(tfidf_matrix[np.nonzero(tfidf_matrix)][:5])\n",
    "# [5.96857651 2.1079054  3.         2.07572071 2.69897   ]\n",
    "print(vocabulary[2000:2010])\n",
    "# ['amoral', 'amorality', 'amorist', 'amorous', 'amorphous', 'amorphously', 'amortization', 'amortize', 'amory', 'amos']\n",
    "print(calculate_sparsity(tfidf_matrix))\n",
    "# 0.9845266994447298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anode', 'space', 'theorem', 'v', 'q', 'c', 'p', 'operator', 'polynomial', 'af']\n"
     ]
    }
   ],
   "source": [
    "U, Sigma, VT = randomized_svd(tfidf_matrix, n_components=10, n_iter=100, random_state=42)\n",
    "salient_words = extract_salient_words(VT, vocabulary, 10)\n",
    "print(salient_words[1])\n",
    "# ['anode', 'space', 'theorem', 'v', 'q', 'c', 'p', 'operator', 'polynomial', 'af']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will fetch documents similar to document 3 - Oslo The most positive element to emerge from the Oslo meeting of North Atlantic Treaty Organization Foreign Ministers has been the freer , franker , and wider discussions , animated by much better mutual understanding than in past meetings . This has been a working session of an organization that...\n",
      "Document 61 is similar to document 3 - For a neutral Germany Soviets said to fear resurgence of German militarism to the editor of the New York Times : For the first time in history the entire world is dominated by two large , powerful nations armed with murderous nuclear weapons that make conventional warfare of the past...\n",
      "Document 6 is similar to document 3 - Resentment welled up yesterday among Democratic district leaders and some county leaders at reports that Mayor Wagner had decided to seek a third term with Paul R. Screvane and Abraham D. Beame as running mates . At the same time reaction among anti-organization Democratic leaders and in the Liberal party...\n"
     ]
    }
   ],
   "source": [
    "print(\"We will fetch documents similar to document {} - {}...\".format(3, ' '.join(documents[3][:50])))\n",
    "# We will fetch documents similar to document 3 - \n",
    "# Oslo The most positive element to emerge from the Oslo meeting of North Atlantic Treaty Organization Foreign Ministers has been the freer , \n",
    "# franker , and wider discussions , animated by much better mutual understanding than in past meetings . This has been a working session of an organization that...\n",
    "\n",
    "similar_doc_indices = get_similar_documents(U, Sigma, VT, 3, 5)\n",
    "for i in range(2):\n",
    "    print(\"Document {} is similar to document 3 - {}...\".format(similar_doc_indices[i], ' '.join(documents[similar_doc_indices[i]][:50])))\n",
    "# Document 61 is similar to document 3 - \n",
    "# For a neutral Germany Soviets said to fear resurgence of German militarism to the editor of the New York Times : \n",
    "# For the first time in history the entire world is dominated by two large , powerful nations armed with murderous nuclear weapons that make conventional warfare of the past...\n",
    "# Document 6 is similar to document 3 - \n",
    "# Resentment welled up yesterday among Democratic district leaders and some county leaders at reports that Mayor Wagner had decided to seek a third term with Paul R. Screvane and Abraham D. Beame as running mates . \n",
    "# At the same time reaction among anti-organization Democratic leaders and in the Liberal party... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will fetch documents relevant to query - Krim attended the University of North Carolina to follow Thomas Wolfe\n",
      "Document 90 is relevant to query - One hundred years ago there existed in England the Association for the Promotion of the Unity of Christendom . Representing as it did the efforts of only unauthorized individuals of the Roman and Anglican Churches , and urging a communion of prayer unacceptable to Rome , this association produced little...\n",
      "Document 96 is relevant to query - Few persons who join the Church are insincere . They earnestly desire to do the will of God . When they fall by the wayside and fail to achieve Christian stature , it is an indictment of the Church . These fatalities are dramatic evidence of `` halfway evangelism ''...\n"
     ]
    }
   ],
   "source": [
    "query = ['Krim', 'attended', 'the', 'University', 'of', 'North', 'Carolina', 'to', 'follow', 'Thomas', 'Wolfe']\n",
    "print(\"We will fetch documents relevant to query - {}\".format(' '.join(query)))\n",
    "relevant_doc_indices = document_retrieval(vocabulary, idf_values, U, Sigma, VT, query, 5)\n",
    "for i in range(2):\n",
    "    print(\"Document {} is relevant to query - {}...\".format(relevant_doc_indices[i], ' '.join(documents[relevant_doc_indices[i]][:50])))\n",
    "# Document 90 is relevant to query - \n",
    "# One hundred years ago there existed in England the Association for the Promotion of the Unity of Christendom . \n",
    "# Representing as it did the efforts of only unauthorized individuals of the Roman and Anglican Churches , and urging a communion of prayer unacceptable to Rome , this association produced little...\n",
    "# Document 101 is relevant to query - To what extent and in what ways did Christianity affect the United States of America in the nineteenth century ? ? \n",
    "# How far and in what fashion did it modify the new nation which was emerging in the midst of the forces shaping the revolutionary age ? ? To what...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".C345",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
