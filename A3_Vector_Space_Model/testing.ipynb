{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "from time import perf_counter\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_unigrams(tweets: list[str], stop_words: list[str], k: int) -> dict[str, int]:\n",
    "    regex = re.compile(r\"^[a-z#].*\")\n",
    "    stop_words = set(stop_words)\n",
    "    \n",
    "    unigram_list = [\n",
    "        word.lower()\n",
    "        for tweet in tweets\n",
    "        for word in tweet.split()\n",
    "        if regex.match(word) and word not in stop_words\n",
    "    ]\n",
    "\n",
    "    top_k_words = Counter(unigram_list)\n",
    "    return top_k_words if k == -1 else dict(top_k_words.most_common(k))\n",
    "\n",
    "\n",
    "def context_word_frequencies(tweets: list[str], stop_words: list[str], context_size: int, frequent_unigrams) -> dict[(str, str), int]:\n",
    "    # Convert to set for O(1) lookups\n",
    "    frequent_unigrams = set(frequent_unigrams) if isinstance(frequent_unigrams, list) else set(frequent_unigrams.keys())\n",
    "    context_pairs = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        # Use numpy array for faster slicing\n",
    "        tokens = np.array(tweet.lower().split())\n",
    "        n = len(tokens)\n",
    "        \n",
    "        # Create all possible context pairs efficiently\n",
    "        for i in range(n):\n",
    "            word1 = tokens[i]\n",
    "            # Calculate context window boundaries\n",
    "            start, end = max(0, i - context_size), min(n, i + context_size + 1)\n",
    "            context = tokens[start:end]\n",
    "            \n",
    "            # Filter context words that are in frequent_unigrams\n",
    "            valid_context = [w for w in context if w in frequent_unigrams and w != word1] # frequent_unigrams is a subset of top_k_words\n",
    "            context_pairs.extend((word1, word2) for word2 in valid_context)\n",
    "    \n",
    "    context_counter = Counter(context_pairs)\n",
    "    return context_counter\n",
    "\n",
    "\n",
    "def pmi(word1: str, word2: str, unigram_counter: dict[str, int], context_counter: dict[(str, str), int]) -> float:\n",
    "    total_unigrams = float(sum(unigram_counter.values()))\n",
    "    total_bigrams = float(sum(context_counter.values()))\n",
    "    \n",
    "    # Get the counts (with pseudo-count = 1 if not observed)\n",
    "    count_w1 = float(unigram_counter.get(word1, 1))\n",
    "    count_w2 = float(unigram_counter.get(word2, 1))\n",
    "    count_w1_w2 = float(context_counter.get((word1, word2), 1))\n",
    "    \n",
    "    p_w1 = count_w1 / total_unigrams\n",
    "    p_w2 = count_w2 / total_unigrams\n",
    "    p_w1_w2 = count_w1_w2 / total_bigrams\n",
    "    \n",
    "    pmi = np.log2(p_w1_w2 / (p_w1 * p_w2))\n",
    "    return pmi\n",
    "\n",
    "\n",
    "def build_word_vector(word1: str, frequent_unigrams, unigram_counter: dict[str, int], context_counter: dict[(str, str), int]) -> dict[str, float]:\n",
    "    frequent_unigrams = set(frequent_unigrams) if isinstance(frequent_unigrams, list) else set(frequent_unigrams.keys())\n",
    "    context_set = set(context_counter.keys())\n",
    "    word_vector = {}\n",
    "\n",
    "    for word2 in frequent_unigrams:\n",
    "        word_vector[word2] = float(0) if (word1, word2) not in context_set else pmi(word1, word2, unigram_counter, context_counter)\n",
    "    \n",
    "    return word_vector\n",
    "\n",
    "\n",
    "def get_top_k_dimensions(word1_vector, k):\n",
    "    sorted_items = sorted(word1_vector.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_k_dimensions = dict(sorted_items[:k])\n",
    "    return top_k_dimensions\n",
    "\n",
    "### TODO: Fix this function\n",
    "def get_cosine_similarity(word1_vector: dict[str, float], word2_vector: dict[str, float]) -> float:\n",
    "    # Convert dictionaries to numpy arrays\n",
    "    vec1 = np.array([word1_vector.get(word) for word in word1_vector.keys()])\n",
    "    vec2 = np.array([word2_vector.get(word) for word in word2_vector.keys()])\n",
    "    \n",
    "    # Use numpy's optimized operations\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "\n",
    "    cosine_sim_score = 0.0 if norm1 == 0 or norm2 == 0 else dot_product / (norm1 * norm2)\n",
    "    return cosine_sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(word2vec: KeyedVectors, word : str, k : int) -> list[(str, float)]:\n",
    "    if word not in word2vec.key_to_index:\n",
    "        return []\n",
    "    # Use gensim's most_similar method as its much faster than calling get_cosine_similarity()\n",
    "    similar_words = word2vec.most_similar(word, topn=k)\n",
    "    return similar_words\n",
    "    \n",
    "\n",
    "def word_analogy(word2vec: KeyedVectors, word1: str, word2: str, word3: str) -> tuple[str, float]:\n",
    "    # Check if all words exist in the model's vocabulary\n",
    "    if not all(word in word2vec.key_to_index for word in [word1, word2, word3]):\n",
    "        return (\"\", 0.0)\n",
    "    \n",
    "    # Doesn't make a call to get_most_similar() as its faster to use gensim's most_similar method\n",
    "    result = word2vec.most_similar(positive=[word2, word3], negative=[word1], topn=1)\n",
    "    word4 = result[0]  # Returns tuple of (word, similarity)\n",
    "    return word4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(A: np.ndarray, B: np.ndarray) -> float:\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm1 = np.linalg.norm(A)\n",
    "    norm2 = np.linalg.norm(B)\n",
    "    cosine_similarity = 0.0 if norm1 == 0 or norm2 == 0 else dot_product / (norm1 * norm2)\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "def get_cos_sim_different_models(word: str, model1: Word2Vec, model2: Word2Vec, cos_sim_function: Callable[[np.ndarray, np.ndarray], float]) -> float:\n",
    "    vec1 = model1.wv[word]\n",
    "    vec2 = model2.wv[word]\n",
    "    cosine_similarity_of_embeddings = cos_sim_function(vec1, vec2)\n",
    "\n",
    "    return cosine_similarity_of_embeddings\n",
    "\n",
    "\n",
    "def get_average_cos_sim(word: str, neighbors: list[str], model: Word2Vec, cos_sim_function: Callable[[np.ndarray, np.ndarray], float]) -> float:\n",
    "    word_vector = model.wv[word]\n",
    "    similarities = []\n",
    "    \n",
    "    for neighbor in neighbors:\n",
    "        try:\n",
    "            neighbor_vector = model.wv[neighbor]\n",
    "            sim = cos_sim_function(word_vector, neighbor_vector)\n",
    "            similarities.append(sim)\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "    return np.mean(similarities) if similarities else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 312877 tweets and 1000 frequent unigrams in 7.13s\n"
     ]
    }
   ],
   "source": [
    "def load_or_compute_variables():\n",
    "    tic = perf_counter()\n",
    "    try:\n",
    "        with open('twitter_analysis_data.pkl', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            print(f\"Loaded {len(data['tweets'])} tweets and {len(data['frequent_unigrams'])} frequent unigrams in {perf_counter() - tic:.2f}s\")\n",
    "            return (data['tweets'], data['stop_words'], data['frequent_unigrams'], \n",
    "                   data['unigram_counter'], data['context_counter'])\n",
    "    except Exception as e:\n",
    "        print(f\"Cache invalid, recomputing: {str(e)}\")\n",
    "        \n",
    "    print(\"Computing fresh values...\")\n",
    "    with open('data/covid-tweets-2020-08-10-2020-08-21.tokenized.txt', 'r', encoding='utf-8') as f:\n",
    "        tweets = [line.strip() for line in f]\n",
    "    with open('data/stop_words.txt', 'r', encoding='utf-8') as f:\n",
    "        stop_words = [line.strip() for line in f]\n",
    "    \n",
    "    frequent_unigrams = top_k_unigrams(tweets, stop_words, 1000)\n",
    "    unigram_counter = top_k_unigrams(tweets, stop_words, -1)\n",
    "    context_counter = context_word_frequencies(tweets, stop_words, 3, frequent_unigrams)\n",
    "    \n",
    "    with open('twitter_analysis_data.pkl', 'wb') as f:\n",
    "        pickle.dump({'tweets': tweets, 'stop_words': stop_words, \n",
    "                    'frequent_unigrams': frequent_unigrams, 'unigram_counter': unigram_counter, \n",
    "                    'context_counter': context_counter}, f)\n",
    "    \n",
    "    print(f\"Time taken: {perf_counter() - tic:.2f}s\")\n",
    "    return tweets, stop_words, frequent_unigrams, unigram_counter, context_counter\n",
    "\n",
    "# global variables\n",
    "tweets, stop_words, frequent_unigrams, unigram_counter, context_counter = load_or_compute_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load Word2Vec model: 31.20 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Exploring Word2Vec\"\"\"    \n",
    "EMBEDDING_FILE = 'data/GoogleNews-vectors-negative300.bin.gz'\n",
    "tic = perf_counter()\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "toc = perf_counter()\n",
    "print(f\"Time taken to load Word2Vec model: {toc - tic:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('respirator', 0.7864563465118408), ('mechanical_ventilator', 0.7063839435577393), ('intensive_care', 0.6809945702552795)]\n",
      "('France', 0.7889978885650635)\n"
     ]
    }
   ],
   "source": [
    "similar_words =  get_most_similar(word2vec, 'ventilator', 3)\n",
    "print(similar_words)\n",
    "# [('respirator', 0.7864563465118408), ('mechanical_ventilator', 0.7063839435577393), ('intensive_care', 0.6809945702552795)]\n",
    "\n",
    "# Word analogy - Tokyo is to Japan as Paris is to what?\n",
    "print(word_analogy(word2vec, 'Tokyo', 'Japan', 'Paris'))\n",
    "# ('France', 0.7889978885650635)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing 40-60 year olds in the 1910s and 40-60 year olds in the 2000s\n",
    "model_t1 = Word2Vec.load('data/1910s_50yos.model')\n",
    "model_t2 = Word2Vec.load('data/2000s_50yos.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8198915917499229\n",
      "0.19302373\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity function for vector inputs\n",
    "vector_1 = np.array([1,2,3,4])\n",
    "vector_2 = np.array([3,5,4,2])\n",
    "cos_similarity = cos_sim(vector_1, vector_2)\n",
    "print(cos_similarity)\n",
    "# 0.8198915917499229\n",
    "\n",
    "# Similarity between embeddings of the same word from different times\n",
    "major_cos_similarity = get_cos_sim_different_models(\"major\", model_t1, model_t2, cos_sim)\n",
    "print(major_cos_similarity)\n",
    "# 0.19302374124526978"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69577473\n",
      "0.27042335\n",
      "0.26262248\n",
      "0.62790346\n"
     ]
    }
   ],
   "source": [
    "# Average cosine similarity to neighborhood of words\n",
    "neighbors_old = ['brigadier', 'colonel', 'lieutenant', 'brevet', 'outrank']\n",
    "neighbors_new = ['significant', 'key', 'big', 'biggest', 'huge']\n",
    "print(get_average_cos_sim(\"major\", neighbors_old, model_t1, cos_sim))\n",
    "# 0.6957747220993042\n",
    "print(get_average_cos_sim(\"major\", neighbors_new, model_t1, cos_sim))\n",
    "# 0.27042335271835327\n",
    "print(get_average_cos_sim(\"major\", neighbors_old, model_t2, cos_sim))\n",
    "# 0.2626224756240845\n",
    "print(get_average_cos_sim(\"major\", neighbors_new, model_t2, cos_sim))\n",
    "# 0.6279034614562988\n",
    "\n",
    "### The takeaway -- When comparing word embeddings from 40-60 year olds in the 1910s and 2000s,\n",
    "###                 (i) cosine similarity to the neighborhood of words related to military ranks goes down;\n",
    "###                 (ii) cosine similarity to the neighborhood of words related to significance goes up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".C345",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
