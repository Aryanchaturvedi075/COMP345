{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryanchaturvedi075/COMP345/blob/main/260976059_Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p_gv9cc0bE8"
      },
      "source": [
        "**NL2DS - Winter 2025**\n",
        "\n",
        "**Assignment 5 -- Psycholinguistic data, sound symbolism, regression, classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DizN-TYp1I1W"
      },
      "source": [
        "Name: **[YOUR NAME HERE]**\n",
        "\n",
        "Student ID: **[YOUR STUDENT ID HERE]**\n",
        "\n",
        "# Instructions\n",
        "\n",
        "This is a long homework, consisting of 78 points + 10 extra credit points.  Different problems/questions will be easier for students with more programming versus more linguistics experience.\n",
        "\n",
        "For questions that require writing code:  \n",
        "  * Replace `# Put your answer here` with your answer.  \n",
        "  * The code block should run when all code above it in this file has also been run.  \n",
        "  * If you skip some problems, it's your responsibility to make sure that all code blocks which you filled out still run.\n",
        "\n",
        "Other questions require writing text.  Replace \"**put your answer here**\" with your answer.\n",
        "\n",
        "For coding questions:\n",
        "* **As a starting point, you might find the Colab notebooks on Regression, Classification and Tree Methods useful for this assignment.**   \n",
        "* <font color='red'>**Do not reimplement any major functionality, such as train/test splits, calculating $R^2$, etc.**</font>\n",
        "* Following the contents of these CoLab notebooks, you should:\n",
        "  * Use `sklearn` functionality as much as possible for machine learning tools. (For example, do not fit a linear regression in Part 1 problems using another Python package.)\n",
        "  * Use `pandas` functionality as much as possible for basic data manipulation and analysis.\n",
        "  * Use the `seaborn` library as much as possible for generating plots.\n",
        "* <font color='red'>**Do not delete any code.  Only add code by replacing `# Put your answer here`. This is important for grading.**</font>\n",
        "\n",
        "**Please make sure to follow directions carefully, including maximum lengths for written answers. Failure to follow directions will result in partial or no credit for the relevant problem/question.**\n",
        "\n",
        "**IMPORTANT: Make sure to correctly follow the instructions at the bottom on submitting your assignment, <font color='red'>INCLUDING MATCHING YOUR ANSWERS TO PDF PAGES WHEN SUBMITTING ON GRADESCOPE</font>. Failure to do so will result in <font color='red'>UP TO 10 POINTS BEING DEDUCTED</font>.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfb1lPqrsNQ1"
      },
      "source": [
        "# Part 1: Regression with psycholinguistic data\n",
        "\n",
        "\n",
        "\n",
        "The first part of this problem set will examine some *lexical decision* data. You can read about lexical decision experiments in the wikipedia article [here](https://en.wikipedia.org/wiki/Lexical_decision_task). (The dataset also contains so-called *speeded naming* data. You can read about that in the speeded naming section of the first paper.)\n",
        "\n",
        "The collection of the lexical decision data is originally described in:\n",
        "\n",
        "Balota, D. A., Cortese, M. J., Sergent-Marshall, S. D., Spieler, D. H., and Yap, M. J. (2004). [Visual word recognition of single-syllable words](https://drive.google.com/file/d/1TkFrg1jg0AK-9ZnKU5YXNVubdVUeNXbL/view?usp=sharing/). *Journal of Experimental Psychology: General*, 133(2):283–316.\n",
        "\n",
        "In the following paper, this data was reanalyzed using some new features (predictors).\n",
        "\n",
        "R. H. Baayen, L. Feldman, and R. Schreuder. [Morphological Influences on the Recognition of Monosyllabic Monomorphemic Words](https://drive.google.com/file/d/1USv3o6PXzocFtSggbcFn2fxYYrJ-pBcI/view?usp=sharing). *Journal of Memory and Language*, 53:496– 512, 2006.\n",
        "\n",
        "This data is discussed in Harald Baayen's book on linguistic data analysis.\n",
        "\n",
        "Baayen, R. H. (2008). [Analyzing Linguistic Data: A practical introduction to statistics](https://drive.google.com/file/d/1RnQLXbzMARsEm8s8PFTYiSlaMc-vZ8Xa/view?usp=sharing). Cambridge University Press.\n",
        "\n",
        "Our data file, `english_a4.csv`, was derived from the original data available as as the `english` dataframe of the [languageR package](https://cran.r-project.org/web/packages/languageR/index.html/).\n",
        "\n",
        "Copy the data to your Drive folder from [here](https://drive.google.com/file/d/1_MgHNXmCiB6bu2PNAaFBYO0rubksw5oF/view?usp=drive_link).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIb1gnEWtEMZ"
      },
      "outputs": [],
      "source": [
        "# throws an error if your Drive folder doesn't contain english_a4.csv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!ls \"/content/drive/My Drive/english_a4.csv\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsGMIIkfG-7X"
      },
      "source": [
        "## Question 1 (2 points)\n",
        "\n",
        "Use [Pandas](https://pandas.pydata.org/docs/user_guide/index.html) to:\n",
        "\n",
        "* Read the CSV file into a DataFrame called `english`.\n",
        "* \"Display\" the dataset, similarly to how we've examined datasets in CoLab notebooks.  The command you use should print the number of rows and columns at the end.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAwSF6fB35qu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Problem 1:\n",
        "\n",
        "# Put your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daqI-lodHGHk"
      },
      "source": [
        "## Question 2 (3 points)\n",
        "\n",
        "You'll first familiarize yourself with the dataset by briefly examining the two papers above.\n",
        "\n",
        "First, read the Wikipedia article on lexical decision, and briefly explain the lexical decision experimental task.  Your answer should address: why do experimenters use this task, what is being measured, and how are conclusions reached on the basis of the results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU6bIwaoHJh9"
      },
      "source": [
        "**Q2: put your answer here (3 sentences max)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg4UggXaHc5x"
      },
      "source": [
        "Now let's turn to the two research papers: Balota et al. (2004) and Baayen et al. (2006).\n",
        "\n",
        "Start with the earlier paper then move on to the later paper. Note these two papers are long and use a lot of technical jargon from the field of psycholinguistics. *Reading each paper carefully would take several hours and you probably would not be able to understand everything unless you have previous familiarity with experimental psychology.*  This is not the goal of this part of the assignment.  The goal is to just familiarize yourself as efficiently as possible with what some of the columns in the data set mean. An important skill in data science is quickly evaluating the high level idea and questions studied in a paper and finding the places where quantitites are defined, without doing a careful reading.\n",
        "\n",
        "A good way to approach this is to first read the abstract, the introduction and the conclusion and then have a look at the figures, always keeping in mind the data from the CSV above and trying to find interpretations for the various columns. Don't get stuck on stuff you don't understand unless you are pretty sure you need to understand it to answer the question.\n",
        "\n",
        "Focus on figuring out where you can find the relevant information to answer the following questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAP_DDUIHdlQ"
      },
      "source": [
        "## Question 3 (2 points)\n",
        "\n",
        "In these studies, using this dataset, various regression models are used to analyze the experimental data. What variable or variables were measured in these studies that corresponds to $\\mathbf{y}$ in our notation from class (i.e., the quantities to be predicted) and which column or columns in the dataset have these values?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiY839_IHd4d"
      },
      "source": [
        "**Q3: put your answer here (2-3 sentences max)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP9rsUg1Hxx2"
      },
      "source": [
        "## Question 4 (4 points)\n",
        "\n",
        "In both papers a number of different quantities are used as predictors (or \"features\") for the experimental measures. These correspond to the columns of our $\\mathbf{X}$ matrix from class, e.g. when we considered linear regression.\n",
        "\n",
        " Note that between these two papers there are a lot of variables, and this a lot of columns in the table. Please determine the meaning of the following features: `Familiarity`, `AgeSubject`, `WordCategory`, `WrittenFrequency`, `WrittenSpokenFrequencyRatio`, `FamilySize`, `InflectionalEntropy`, `LengthInLetters`, `Voice`.  You will be graded on a random subset of your descriptions (about half).\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcSHrc9GHyHl"
      },
      "source": [
        "**Q4: put your answer here (make your answer an itemized list, with one item per predictor, consisting of one sentence per predictor)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKDD5O8rKBba"
      },
      "source": [
        "## Question 5 (3 points)\n",
        "\n",
        "The largest effect in this data is age: younger participants have lower reaction times. Some predictors' effects may in fact differ between younger and older participants. To abstract away from this for this assignment, we will restrict to just data from younger participants.\n",
        "\n",
        "We will also abstract away from the fact that a couple of the predictors here, `WordCategory` and `Voice`, are categorical. Instead we'll code them as 0/1 valued, so that:\n",
        "\n",
        "* `WordCategory` = *N* / *V* becomes 0/1\n",
        "* `Voice` = *voice* / *voiceless* becomes 0/1\n",
        "\n",
        "Let's simplify the dataset as follows, saving to a new dataframe called `english_young`:\n",
        "\n",
        "* Drop rows which don't correspond to young speakers, then drop the column indexing whether speakers are old or young.\n",
        "* Keep the column for lexical decision RT, which will be our $\\mathbf{y}$, and drop any other columns that are possible outcome variables (from your answer to Question 2).\n",
        "* Keep the column for `Word`, which tells us what word (of English) each row corresponds to.\n",
        "* Recode the `WordCategory` and `Voice` columns as numeric, as specified above.\n",
        "* Keep columns corresponding to the remaining predictors from Question 4.\n",
        "* Drop all other columns.\n",
        "\n",
        "Then, print a one-line message giving the number of rows and columns in `english_young`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPYZ3d98Ke-8"
      },
      "outputs": [],
      "source": [
        "# Question 5:\n",
        "\n",
        "\n",
        "## simplify data\n",
        "\n",
        "# subset to young speakers\n",
        "\n",
        "# Put your answer here\n",
        "\n",
        "\n",
        "# restrict to certain columns\n",
        "\n",
        "# Put your answer here\n",
        "\n",
        "\n",
        "\n",
        "## map categorical predictors to numeric\n",
        "category_to_number_dict = {'N': 0,'V': 1, 'voiced': 0,'voiceless': 1}\n",
        "\n",
        "english_young['WordCategory'] = english_young['WordCategory'].map(category_to_number_dict)\n",
        "english_young['Voice'] = english_young['Voice'].map(category_to_number_dict)\n",
        "\n",
        "## print message\n",
        "\n",
        "# Put your answer here\n",
        "\n",
        "\n",
        "#######\n",
        "display(english_young)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxt-bJAfNUzQ"
      },
      "source": [
        "We now use the [Seaborn library](https://seaborn.pydata.org/) to produce a set of plots between (see `pairplot`) all the variables in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6thJWFlRAFT_"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns; sns.set()\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## kind = 'reg': add linear trend lines\n",
        "## diag_kind = 'kde' : show density plots for each predictor in diagonal panels.\n",
        "sns.pairplot(english_young, kind = 'reg', diag_kind='kde')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnt9epOeRcVS"
      },
      "source": [
        "## Question 6 (4 points)\n",
        "\n",
        "Let's examine the relationship between the written frequency of a word and its\n",
        "lexical decision time.  \n",
        "\n",
        "When examining relationships between two variables, especially when we're not sure if they're linear, it's useful to look at a *locally-smoothed regression line* that relates the $x$ and $y$ axes of a plot. This is a kind of regression model where the function is refit localy for many subsets of the data then a smooth line is interpolated between these points. One standard technique for this is known as *locally weighted scatterplot smoothing* or [LOWESS](https://en.wikipedia.org/wiki/Local_regression).\n",
        "\n",
        "When examining large datasets like this one, it's important to format how the data is displayed so that both the empirical distribution of data and the fitted trend  (here, linear or LOWESS line) are legible, meaning:\n",
        "* Points should not overlap too much\n",
        "* Neither points nor the trend is formatted such that the other is obscured.\n",
        "\n",
        "Other desiderata for any plot are:\n",
        "* x and y axes should be clearly labeled (with interpretable labels, not variable names like `RTlexdec`)\n",
        "* Text should be legible: appropriately-sized fonts, no overlapping text.\n",
        "\n",
        "Use functions from matplotlib and seaborn to make **legible** plots meeting the specifications above:\n",
        "\n",
        "* Make a 1 x 2 grid of plots\n",
        "* In the left plot, put a scatterplot of written frequency (x-axis) and lexical decision RT (y-axis), with a superimposed linear trend (line of best fit).  \n",
        "* In the right plot, put a scatterplot of written frequency (x-axis) and lexical decision RT (y-axis), with a superimposed LOESS of best fit.\n",
        "* In both plots: adjust the size, transparency, and/or color of the lines and/or dots as appropriate.\n",
        "\n",
        "You may find the Seaborne help pages useful, such as [this one](https://seaborn.pydata.org/tutorial/regression.html).  Some possible functions to use:\n",
        "\n",
        "* `plt` and `plt.subplots` from `matplotlib.pyplot`\n",
        "* `regplot` from `seaborn`\n",
        "\n",
        "**See 'Visualization' in the 'Regression' Colab Notebook (used in class, and found under MyCourses -> Content -> Code workbooks) for an example of code that creates legible plots. <font color='red'>Points will be deducted for plots that are not sufficiently legible as per the details mentioned above.</font>**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAHZ1-WJQJVa"
      },
      "outputs": [],
      "source": [
        "# Question 6:\n",
        "# Put your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFvnlwr8-hFO"
      },
      "source": [
        "## Question 7 (2 points)\n",
        "\n",
        "Based on these two plots, do you think that a linear model represents the relationship between written frequency and reaction time?  Why/why not? If we fit a polynomial approximation of order $k$ to the LOESS curve, what $k$ do you think would be most appropriate? You can specify up to two possible $k$ values (e.g. \"k = 3\" or \"$k =$ 1--2\" is OK, \"$k =$ 3, 5 or 9\" is not).  Your answer should be verbal, with your guess at $k$ purely based on visual inspection. See 'Polynomial Regression' in the Regression Colab Notebook (MyCourses -> Content -> Code workbooks) to see what different $k$ values look like.\n",
        "\n",
        "NB: A line is a polynomial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r83VtVN4IWVv"
      },
      "source": [
        "**Q7: put your answer here (2-3 sentences max)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RK7sh2H0T9m"
      },
      "source": [
        "## Question 8 (2 points)\n",
        "\n",
        "When modeling any relationship in data, it's important to think not just about what quantitative model (e.g. a line vs. a LOWESS curve) fits best, but what  relationships are possible given domain-specific knowledge.\n",
        "\n",
        "Let's consider the linear fit from this perspective.  Think about what a linear fit predicts for reaction time as written frequency is changed, and what people are doing in a lexical decision task. Is there any issue (or multiple issues) that tells us that the true relationship cannot be linear? Explain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk7i9Wzs2Al6"
      },
      "source": [
        "\n",
        "**Q8: put your answer here (2-3 sentences max)**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEgsg-YAFhPR"
      },
      "source": [
        "# Question 9 (2 points), Question 10 (6 points)\n",
        "\n",
        "We'll now check your intuition from above by examining more complex models of the relationship between frequency and lexical decision time, similarly to cases in the Regression CoLab notebook considered in class.\n",
        "\n",
        "Fill in the following code for fitting polynomial regressions of degree $k$, choosing the best $k$, and visualizing the resulting relationship.\n",
        "\n",
        "The one difference from the code considered in class is that we will consider two measures of goodness of fit:\n",
        "\n",
        "1. $R^2$ on the test set\n",
        "2. Bayesian Information Criterion (BIC) on the test set\n",
        "\n",
        "Note that as defined here, **lower** BIC = better model (lower value, not lower absolute value; a BIC of -1500 is lower than a BIC of -1000).\n",
        "\n",
        "*Hint*: Do not implement your own function for train/test splitting, or for computing polynomial components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac5k9q16XL4w"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries and defining BIC function:\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import random\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "def bic(X, y, degree, model):\n",
        "  # number of observations\n",
        "  n = X.shape[0]\n",
        "\n",
        "  # number of parameters\n",
        "  k = degree + 1\n",
        "\n",
        "  # calculate Residual Sum of Squares)\n",
        "  RSS = mean_squared_error(y, model.predict(X)) * n\n",
        "\n",
        "  BIC = n * np.log(RSS / n) + k * np.log(n)\n",
        "\n",
        "  return(BIC)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IflttmBf4ElN"
      },
      "outputs": [],
      "source": [
        "# Question 9: Preprocessing\n",
        "\n",
        "# starting from english_young:\n",
        "# - Set up a predictor matrix X for features -- considering just the written frequency feature\n",
        "# - Set up the outcome vector, y.\n",
        "\n",
        "# Put your answer here\n",
        "\n",
        "\n",
        "# - Split the data into train and test subsets, with 20% of the data in test.\n",
        "# This should define objects called X_train, X_test, y_train, and y_test.\n",
        "\n",
        "# Put your answer here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7Fs2fim4WN3"
      },
      "outputs": [],
      "source": [
        "## Question 10: polynomial regression + visualization\n",
        "\n",
        "######\n",
        "# Sets up a scatterplot of training data:\n",
        "X_plot = np.linspace(0, 10,5000).reshape(-1, 1)\n",
        "plt.scatter(X_train, y_train, color='blue', alpha=0.1)\n",
        "\n",
        "######\n",
        "print(\"Model class: \" + \"Linear Regression\")\n",
        "for degree in [1,2,3,4,5,6,7,10,25]:\n",
        "  # - fit a polynomial regression model with this degree, on the training data\n",
        "  # it should be named 'model'.\n",
        "\n",
        "  # Put your answer here\n",
        "\n",
        "  print(\"\\tDegree \" + str(degree) +\"\\n\\t\\tTrain R^2: \"+ str(model.score(X_train,y_train)))\n",
        "  print(\"\\t\\tTest R^2: \"+ str(model.score(X_test,y_test)))\n",
        "  print(\"\\t\\tBIC: \"+ str(bic(X_test, y_test, degree, model)))\n",
        "\n",
        "\n",
        "  # Add a line to the plot for this model, showing predictions for the test data.\n",
        "  # Note: your plots will be messy and unintuitive if you don't make sure to sort the vectors you provide to the plotting function.\n",
        "  #       Make sure to sort your X_test vector before passing it to the model, and then plotting the predictions.\n",
        "\n",
        "  # Put your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBZITMDdHB9Q"
      },
      "source": [
        "## Question 11 (3 points)\n",
        "\n",
        "Which degree polynomial provided the best fit to this dataset based on $R^2$?  Based on BIC?  Which answer makes more sense given your answers to Questions 7 and 8?  Is the relationship between frequency and lexical decision time linear or nonlinear?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQVNZhw-KqMX"
      },
      "source": [
        "**Q11: put your answer here (3 sentences max)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SI-LDnZcSyj"
      },
      "source": [
        "## Question 12 (4 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNxiJbLO_CBX"
      },
      "source": [
        "Let's now fit a model using all predictors, including a polynomial effect of `WrittenFrequency`, of the degree you chose in Question 11.\n",
        "\n",
        "For interpreting the model coefficients, it's useful to first standardize both $y$ and the columns of X.  \n",
        "\n",
        "Prepare the data for this model:\n",
        "\n",
        "*Hint*: Do not implement your own function for z-scoring each column of a DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoeTiyfaTM57"
      },
      "outputs": [],
      "source": [
        "# Question 12\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "## define X such that the columns are predictor variables in english_young,\n",
        "## with columns added for polynomial features.\n",
        "##\n",
        "## for example, if you found in Question 6 that k = 4, then you'd add a\n",
        "## columns here called WrittenFrequency2, which is the square of the WrittenFrequency column,\n",
        "## and similarly for WrittenFrequency3 and WrittenFrequency4\n",
        "\n",
        "# Your code here\n",
        "\n",
        "## Now: define X_std and Y_std:\n",
        "## - X_std is the X matrix above, but with each column z-scored\n",
        "## - y_std is the same as y above, but z-scored\n",
        "\n",
        "# Your code here\n",
        "\n",
        "# - Split the z-scored data into train and test subsets, with 20% of the data in test.\n",
        "# This should define objects called X_std_train, X_std_test, y_std_train, and y_std_test.\n",
        "\n",
        "# Your code here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKkvnyRLUeXC"
      },
      "source": [
        "# Question 13 (3 points)\n",
        "\n",
        "There are many predictors here, some of which probably don't actually have non-zero effects.  We'll fit a Lasso regression, which should perform as well as linear regression, while allowing us to perform variable selection.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipBm7EhcUdbP"
      },
      "outputs": [],
      "source": [
        "## Question 13\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Fit a Lasso linear regression to X_std_train and y_std_train (which correspond to the train split of the X_std and y_std data), with alpha parameter of 0.02.\n",
        "# (you can just assume this is a good alpha).  Call this model mod_lasso.\n",
        "\n",
        "# Your code here\n",
        "\n",
        "\n",
        "# Print the R^2 of this model on the train and test set\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYWMs4LKUdt2"
      },
      "source": [
        "## Question 14 (3 points)\n",
        "\n",
        "Print out a pandas DataFrame summarizing the coefficient value for each predictor for this model, called `coefficients_with_features`. Column 1 should be predictor names and Column 2 coefficient values.  The rows of the table should be sorted in order of coefficient magnitudes (= absolute values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnS1J6unMk7v"
      },
      "outputs": [],
      "source": [
        "## Question 14\n",
        "\n",
        "# extract model coeffs\n",
        "# Your code here\n",
        "\n",
        "# make the dataframe\n",
        "# Your code here\n",
        "\n",
        "# sort the DataFrame by coefficient magnitude\n",
        "# Your code here\n",
        "\n",
        "print(coefficients_with_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ma-btGsWDyL"
      },
      "source": [
        "## Question 15 (2 points)\n",
        "\n",
        "According to the Lasso regression:\n",
        "* Which two predictors have the largest effects?\n",
        "* Which predictors are selected as having no effect?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vddr4KU8WEKh"
      },
      "source": [
        "**Q15: put your answer here (2 sentences max)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b01wKhlWBvE"
      },
      "source": [
        "## Question 16 (3 points)\n",
        "\n",
        "You should find that two of the predictors that have large effects are very correlated (see the empirical plot above, between Question 5 and 6). Call these $x_1$ and $x_2$.  What are $x_1$ and $x_2$? (Choose the most-correlated pair of predictors.)\n",
        "\n",
        "Suppose that in reality, only $x_1$ (causally) affects `RTlexdec`, and $x_2$ just looks correlated with `RTlexdec` because it's highly correlated with $x_1$. Why hasn't Lasso selected $x_2$ as having no effect (and will not do so, even if we increase `alpha`)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfTov9zQKuip"
      },
      "source": [
        "**Q16: put your answer here (3-4 sentences max)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idBNS_7VWvWQ"
      },
      "source": [
        "## Question 17 (3 points)\n",
        "\n",
        "* Why is $R^2$ on the test set lower than on the training set?\n",
        "* If the `alpha` parameter were increased, would we expect the $R^2$ on the test set to increase or decrease? Do we expect more or fewer predictors to be selected as having no effect? Explain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wg8ZjoTXFak"
      },
      "source": [
        "**Q17: put your answer here (3 sentences max)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVktxY2RYvpj"
      },
      "source": [
        "# Part 2: Classification with Pokémon data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEiKECeSYzEa"
      },
      "source": [
        "This part uses Pokémon name data to examine sound symbolism: to what extent are properties of a Pokémon predictable from its name?  We will be considering *evolution*, a fundamental division between Pokémon characters.  For our purposes, Pokémon can be either *evolved* or *non-*evolved. (The real story is [more complicated](https://bulbapedia.bulbagarden.net/wiki/Evolution), as many of you know, but this is a reasonable first approximation.)\n",
        "\n",
        " An interesting aspect of Pokémon for linguistic research is that complete Pokémon name sets exist in different languages, giving us multiple datasets to examine sound symbolism and to what extent it looks similar across languages.\n",
        "\n",
        "In class we examined Pokémon evolution status as a classification problem for English names .  In this homework, we'll do the same for Mandarin Chinese names (henceforth \"Mandarin\").\n",
        "\n",
        "This data comes from a recent paper:\n",
        "\n",
        "Kilpatrick, A., Ćwiek, A., and Kawahara, S. (2023). [Random forests, sound symbolism and Pokémon evolution](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0279350).\n",
        "PLoS ONE 18(1): e0279350. https://doi.org/10.1371/journal.pone.0279350\n",
        "\n",
        "This paper considers Korean, Japanese, and Mandarin datasets, all available in this [OSF project](https://osf.io/pe24w/?view_only=02e9327a7bd54b9280b57434a90ed83a).  \n",
        "\n",
        "The datafile we are using, `chinese_pokemon.csv`, is derived from the data on this site.\n",
        "\n",
        "Copy the data to your Drive folder from [here](https://drive.google.com/file/d/1jA5M38-v7gNzQbacihnp7aItAycM_89p/view?usp=sharing).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cg3PFlOScUgJ"
      },
      "outputs": [],
      "source": [
        "# throws an error if your Drive folder doesn't contain chinese_pokemon.csv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!ls \"/content/drive/My Drive/chinese_pokemon.csv\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mNcXN7UcXkY"
      },
      "source": [
        "First, load the data and take a look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGhwEIl4cYMv"
      },
      "outputs": [],
      "source": [
        "chinese = pd.read_csv(\"/content/drive/My Drive/chinese_pokemon.csv\")\n",
        "display(chinese)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyjZMDNyckgY"
      },
      "source": [
        "Column meanings:\n",
        "\n",
        "* `name`: Pokémon's name, in a custom transcription system$^{*}$\n",
        "* `length`: length of name, in phones\n",
        "* `evolved`: evolved Pokémon? 0/1 = False/True\n",
        "* `flat_tone`, `rising_tone`, etc.: number of syllables in the name carrying this tone\n",
        "* `a`, `i`, `e`, etc: number of times this phone appears in the name\n",
        "\n",
        "The transcription system used is close to [Pinyin](https://en.wikipedia.org/wiki/Pinyin), but modified so that every phoneme is represented by a single ASCII character---similar to the X-SAMPA system for English used in our `h95.csv` vowels dataset.  (If you are curious / familiar with Mandarin, the system is described on p. 5 of [this document](https://osf.io/32whg?view_only=02e9327a7bd54b9280b57434a90ed83a).)  Some things you may need to know for this homework are:\n",
        "\n",
        "* Every syllable in Mandarin bears one of 5 tones: flat, rising, falling-rising, falling, or neutral.\n",
        "* `U` stands for a front rounded vowel (written \"ü\" in Pinyin)\n",
        "* `N` stands for the velar nasal, which can only occur at the end of syllables in Mandarin (written \"ng\" in Pinyin).\n",
        "* `j` stands for the affricate /tɕ/ (written \"j\" in Pinyin).\n",
        "* `y` stands for the glide /j/ (written \"y\" in Pinyin).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11xSOXgYdS-y"
      },
      "source": [
        "## Question 18 (2 points)\n",
        "\n",
        "Prepare the data:\n",
        "\n",
        "* Make the predictor matrix: a numpy DataFrame `X` which consists of all columns except `name` and `evolved`.\n",
        "* Make the outcome vector `y`\n",
        "*  Split the data into train and test subsets, with 20% of the data in test. This should define objects called `X_train`, `X_test`, `y_train`, and `y_test`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcStaWxLPWU6"
      },
      "outputs": [],
      "source": [
        "# Question 18\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1enAx8KdqTf"
      },
      "source": [
        "\n",
        "We will fit two classification models to this dataset, with the goal of determining which predictors (properties of a Pokémon's name) affect `evolution`.  \n",
        "\n",
        "Some background on sound symbolism will be useful.  We might hypothesize that \"evolved\" status would be correlated with some types of sounds which have been found to evoke large size/heaviness/hardness across languages:\n",
        "\n",
        "* Low vowels, such as `a`: positive correlation\n",
        "* High vowels, such as `i`: negative correlation\n",
        "* Back vowels, such as `u`: negative correlation\n",
        "* Nasal consonants, especially in syllable codas: positive correlation\n",
        "* Bilabial consonants: negative correlation\n",
        "\n",
        "One theory underlying such associations is Ohala's *frequency code hypothesis*, which posits that sounds that tend to have higher f0 (pitch) are more associated with greater size/weight/male gender.  \n",
        "\n",
        "For Pokémon names, it is well known (by players) that:\n",
        "\n",
        "*  *longer names* are positively correlated with \"evolved\" status (as well as higher power).  \n",
        "\n",
        "This pattern seems to be Pokémon-specific sound symbolism.\n",
        "\n",
        "Interestingly, not much is known about sound symbolism involving tones across languages, including in Mandarin Chinese (the world's most-spoken tone language)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C2lArCyQMGp"
      },
      "source": [
        "## Question 19 (3 points)\n",
        "\n",
        "We will first fit and evaluate a logistic regression model to predict `evolved`.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1Np7Pcadp1U"
      },
      "outputs": [],
      "source": [
        "# Problem 12\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Fit a logistic regression model, called lr_model, to X_train and y_train.\n",
        "# Make sure that the model does not use any regularization -- the\n",
        "# sklearn default includes L2 regularization.\n",
        "\n",
        "# Your code here\n",
        "\n",
        "\n",
        "# Calculate the accuracy on the training set and on the test set.\n",
        "# save these as train_acc and test_acc\n",
        "\n",
        "# Your code here\n",
        "\n",
        "# print these accuracies:\n",
        "print([train_acc, test_acc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOFFQgmKVE19"
      },
      "source": [
        "## Question 20 (3 points)\n",
        "\n",
        "To examine which predictors are important, print a table of coefficients where:\n",
        "* Each row corresponds to one predictor\n",
        "* Column 1: predictor names\n",
        "* Column 2: coeficient values\n",
        "* Rows sorted by decreasing coefficient _absolute value_.\n",
        "\n",
        "Fill in the missing parts of code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UN9M07apyhf"
      },
      "outputs": [],
      "source": [
        "## Question 20\n",
        "\n",
        "# make 'feature_names' the names of columns of X_train\n",
        "# make 'coefficients' a numpy array consisting of the values of the\n",
        "# coefficients of lr_model\n",
        "\n",
        "# Your code here\n",
        "\n",
        "\n",
        "#  Make a DataFrame:\n",
        "coef_table = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# sort the Dataframe by absolute value of coefficients, then print it.\n",
        "\n",
        "# Your code here\n",
        "\n",
        "print(coef_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evlJCO4ZWkiM"
      },
      "source": [
        "## Question 21 (2 points)\n",
        "\n",
        "What are the four most important features, going by coefficient values?  Briefly describe what they mean (e.g. `a` would be \"number of times 'a' appears in the name\").\n",
        "\n",
        "**Q21: put your answer here, with each feature one line of an itemized list. (4 sentences)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNgapHVRfZmb"
      },
      "source": [
        "## Question 22 (3 points)\n",
        "\n",
        "Our second model will be a random forest. Fit a random forest called `rf` to the training data, with the following options:\n",
        "\n",
        "* Minimum number of samples per split: 5\n",
        "* Maximum tree depth: 5\n",
        "* Use OOB score instead of accuracy\n",
        "* Use 1000 trees\n",
        "\n",
        "(These options make this particular random forest perform better, and you can just take them as given.)\n",
        "\n",
        "Then print its accuracy on the train and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPDQu-BEfaZE"
      },
      "outputs": [],
      "source": [
        "## Question 22\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X43qPBNZfm8o"
      },
      "source": [
        "To compute feature importances for this random forest, we'll work from [this sklearn vignette](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html).\n",
        "\n",
        "We will compute *permutation importance* on a held-out test set, as in the example shown after the paragraph beginning with \"As an alternative, the permutation importances of rf are computed on a held out test set...\" Read as much of the vignette as necessary to understand what is being done here, and what the boxplots in the following plot mean. (Why does the figure show a range of values for each feature, rather than just a single importance number?)\n",
        "\n",
        "We adapt the code there to calculate permutation importance and show a plot of horizontal boxplots, like the one shown there:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO1wDt11fweq"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# This code will work after you've defined rf, but will take a while to run\n",
        "\n",
        "## calculate permutation importance\n",
        "result = permutation_importance(\n",
        "    rf, X_test, y_test, n_repeats=50, random_state=42, n_jobs=-1\n",
        ")\n",
        "\n",
        "## arrange as a dataframe, sorted by importance\n",
        "sorted_importances_idx = result.importances_mean.argsort()\n",
        "importances = pd.DataFrame(\n",
        "    result.importances[sorted_importances_idx].T,\n",
        "    columns=X.columns[sorted_importances_idx],\n",
        ")\n",
        "\n",
        "# plot importances on the test set\n",
        "ax = importances.plot.box(vert=False, whis=10)\n",
        "ax.set_title(\"Permutation Importances (test set)\")\n",
        "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
        "ax.set_xlabel(\"Decrease in accuracy score\")\n",
        "ax.figure.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuWsL3u2ZVp1"
      },
      "source": [
        "## Question 23 (2 points)\n",
        "\n",
        "What are the four most important features, going by this plot?  How much do these features overlap with those from Question 21?\n",
        "\n",
        "\n",
        "**Q23: put your answer here (3 sentences max)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tdYv30IkhML"
      },
      "source": [
        "## Question 24 (4 points, up to 4 points extra credit)\n",
        "\n",
        "To get a sense of how each of these features affects `evolved`: for each feature, make four empirical plots: one for each feature, with the feature on the x-axis and % evolved on the y-axis.  These plots should be in a 1x4 grid.\n",
        "\n",
        "Each plot can just show one point per value of the feature, corresponding to the % of the data with this feature value (e.g. `a` = 2) for which `evolved` is 1.  \n",
        "\n",
        "Your plots should be **legible**, following the guidelines in Question 6, though it's not required to show the empirical data in the plots.\n",
        "\n",
        "*Extra credit*: calculate the error for each % evolved, and showing these on the plots (using 95% confidence intervals).  Add information to the plot showing the empirical data: the number of points with `evolved` = 1 vs. 0 for each feature value. Just using a default scatterplot isn't informative (why?)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZwyhOksaP3N"
      },
      "outputs": [],
      "source": [
        "## Question 24\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIhvKZYonB34"
      },
      "source": [
        "## Question 25 (4 points)\n",
        "\n",
        "Using your plots from Question 22 and the results of Question 23, discuss your findings from the random forest with respect to the sound symbolism background above (see Question 18).  Be sure to consider at least one feature you do _not_ find to be informative.\n",
        "\n",
        "**Q25: put your answer here (3-4 sentences max)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHcj6Cs7Ct68"
      },
      "source": [
        "## Question 26 (4 points)\n",
        "\n",
        "Compare your findings from Part 2 (i.e. Questions 18-25) with the findings presented in class, from the Regression Colab Notebook (MyCourses -> Content -> Code workbooks). Did you find that similar features predict (i) `power` when using English names (this is what is covered in class, and in the Regression Colab Notebook); and (ii) `evolved` status when using Mandarin names (this is what is covered from Questions 18-25 of this assignment)? Mention any similarities and differences you find."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4BHzzMWDzhq"
      },
      "source": [
        "**Q25: Put your answer here (4 sentences max)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZALSpgJahPk"
      },
      "source": [
        "## Question 27 (Extra Credit: 6 points)\n",
        "\n",
        "You should find that the most-informative features are quite different for the logistic regression and random forest models.  For the top two features listed as informative by the logistic regression model but not the RF model:\n",
        "\n",
        "* Figure out why the LR but not the RF model has chosen them as informative.\n",
        "* Explain why the RF model _doesn't_ choose them as informative.\n",
        "* Explain why the RF's behavior is preferable.\n",
        "\n",
        "A full answer will require writing both code and prose.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIR66UqNaGeo"
      },
      "outputs": [],
      "source": [
        "## Question 27\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-kHiSdzaIEl"
      },
      "source": [
        "**Extra Credit: put your answer here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL7VRHX3CE6_"
      },
      "source": [
        "# To Submit\n",
        "To submit:\n",
        "* Name this notebook `YOUR_STUDENT_ID_Assignment_5.ipynb` and download it.\n",
        "* Convert this `.ipynb` file to a `.pdf` (e.g., using the following instructions).  \n",
        "* Upload the PDF to the Gradescope assignment \"Assignment 5\".\n",
        "     * **<font color='red'>Make sure to match your answers to page numbers when submitting the PDF on Gradescope. Failure to do so will result in UP TO 10 POINTS BEING DEDUCTED.</font>**\n",
        "* Submit the `.ipynb` file on myCourses under Assignment 5.\n",
        "\n",
        "(Note: `Print > Save as PDF` **will not work** because it will not display your figures correctly.)\n",
        "\n",
        "You can convert the notebook to a PDF using the following instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_PZj4fgA70w"
      },
      "source": [
        "# Converting this notebook to a PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEUQt-EbA_mv"
      },
      "source": [
        "1. Make sure you have renamed the notebook, e.g. `000000000_Assignment_5.ipynb` where `000000000` is your student ID.\n",
        "2. Make sure to save the notebook (`ctrl/cmd + s`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7HeqYFKBKL0"
      },
      "source": [
        "2. Make sure Google Drive is mounted (it likely already is from the first question)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgOBFJCwBPG3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDsj2M8hBOce"
      },
      "source": [
        "3. Install packages for converting .ipynb to .pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUv9uHo2CCzy"
      },
      "outputs": [],
      "source": [
        "!apt-get -q install texlive-xetex texlive-fonts-recommended texlive-plain-generic\n",
        "!apt-get install -y pandoc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X7rUukBCKlu"
      },
      "source": [
        "4. Convert to PDF (replace `000000000` with your student ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwMbBqmaCH8M"
      },
      "outputs": [],
      "source": [
        "%env STUDENT_ID=000000000\n",
        "!jupyter nbconvert --to pdf \"/content/drive/My Drive/Colab Notebooks/${STUDENT_ID}_Assignment_5.ipynb\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4fwKz3SCSGa"
      },
      "source": [
        "5. Download the resulting PDF file. If you are using Chrome, you can do so by running the following code. On other browsers, you can download the PDF using the file mananger on the left of the screen (Navigate to the file > Right Click > Download)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7uiHC2uCg9-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "files.download(f\"/content/drive/My Drive/Colab Notebooks/{os.environ['STUDENT_ID']}_Assignment_5.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvvxRqqTDAFT"
      },
      "source": [
        "6. Verify that your PDF correctly displays your figures and responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Ew7g-5cW7m"
      },
      "source": [
        "7. **<font color='red'>Remember to match your answers to page numbers when submitting the PDF on Gradescope!</font>**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
